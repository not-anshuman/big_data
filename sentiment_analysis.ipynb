{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAH2ERd68HDj"
      },
      "source": [
        "## Twitter sentiment analysis and prediction using pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9EBWw2C8H0Y",
        "outputId": "c8379df5-2360-419f-c81c-aa65e40d809a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=bb0d31e1278c5cbfe3a8dee587e88627ceee586989c1446842f79402f8bef974\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "#Install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xWPMaJM88HDk"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdAhEF4u8HDl"
      },
      "source": [
        "### Create Spark Context and load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnvkpD4V8HDl",
        "outputId": "3b47e8e8-cd25-46a9-870f-c6681d19c1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext()\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SOBXV2o98HDm"
      },
      "outputs": [],
      "source": [
        "customSchema = StructType([\n",
        "    StructField(\"clean_text\", StringType()), \n",
        "    StructField(\"category\", StringType())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw9kMFkkD4Ea",
        "outputId": "dca5caff-0a9e-4e23-ed79-6ca4eb3f9da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qIZgUqNW8HDm"
      },
      "outputs": [],
      "source": [
        "#Import twitter and reddit datasets\n",
        "filename1 = '/content/drive/MyDrive/BigDataProject/twtr_dataset.csv'\n",
        "filename2 = '/content/drive/MyDrive/BigDataProject/redt_dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNm462LR8HDm",
        "outputId": "1cc94f24-9c68-4eb4-982e-903ef6b58782"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192131"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df1 = sqlContext.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename1)\n",
        "df1.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dt0fU5A8HDm",
        "outputId": "ac4d23f1-1065-49bc-92db-18c457754716"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38305"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df2 = sqlContext.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename2)\n",
        "df2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCMgNzKz8HDn",
        "outputId": "3bd1b608-df3d-4cc2-847d-e3e6ab822ed1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38305"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#df = df1.union(df2)\n",
        "df = df2\n",
        "df.count() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPvdxEjI8HDn",
        "outputId": "72bcbfd5-2bfd-42b7-9509-68c1f912a45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+\n",
            "|          clean_text|category|\n",
            "+--------------------+--------+\n",
            "| family mormon ha...|       1|\n",
            "|buddhism has very...|       1|\n",
            "|seriously don say...|      -1|\n",
            "|what you have lea...|       0|\n",
            "|for your own bene...|       1|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = df.na.drop(how='any')\n",
        "data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLvpi7Zt8HDn",
        "outputId": "f8609691-e3af-4a37-d433-c919b926feaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38305"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.count() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5u_tSty8HDn",
        "outputId": "a6be67d2-e6b1-4fc1-9f3c-ca225642ea24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- clean_text: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCWIFjQe8HDn"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rjRvPhi8HDn",
        "outputId": "9f7959da-3ffe-4613-db4c-7f4741e9088b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|       1|15749|\n",
            "|       0|12895|\n",
            "|      -1| 8244|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "data.groupBy(\"category\").count().orderBy(col(\"count\").desc()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ePiGt9F8HDn"
      },
      "source": [
        "# Preprocessing of Tweets\n",
        "\n",
        "\n",
        "*   Remove punctuations and alphanumeric characters\n",
        "*   Remove hyperlinks\n",
        "*   Remove mentions\n",
        "*   Convert text to lower case\n",
        "\n",
        "\n",
        "*   Remove stopwords\n",
        "*   Fix abbreviated text\n",
        "*   Part of Speech Tagging\n",
        "\n",
        "\n",
        "\n",
        "*   Lemmatization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gh0Ns6O5LQxj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "import re\n",
        "import string\n",
        "\n",
        "##Remove punctuations mentions and alphanumeric characters\n",
        "def remove_features(data_str):\n",
        "# compile regex\n",
        "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
        "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    num_re = re.compile('(\\\\d+)')\n",
        "    mention_re = re.compile('@(\\w+)')\n",
        "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
        "# convert to lowercase\n",
        "    data_str = data_str.lower()\n",
        "# remove hyperlinks\n",
        "    data_str = url_re.sub(' ', data_str)\n",
        "# remove @mentions\n",
        "    data_str = mention_re.sub(' ', data_str)\n",
        "# remove puncuation\n",
        "    data_str = punc_re.sub(' ', data_str)\n",
        "# remove numeric 'words'\n",
        "    data_str = num_re.sub(' ', data_str)\n",
        "# remove non a-z 0-9 characters and words shorter than 1 characters\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    for word in data_str.split():\n",
        "        if list_pos == 0:\n",
        "            if alpha_num_re.match(word):\n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = ' '\n",
        "        else:\n",
        "            if alpha_num_re.match(word):\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "            else:\n",
        "                cleaned_str += ' '\n",
        "        list_pos += 1\n",
        "# remove unwanted space, *.split() will automatically split on\n",
        "# whitespace and discard duplicates, the \" \".join() joins the\n",
        "# resulting list into one string.\n",
        "    return ' '.join(cleaned_str.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMlc2yiYKWhr",
        "outputId": "ee560aa8-49b8-48d3-9b74-1fe35d33e5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xf8ZOXChNPLc"
      },
      "outputs": [],
      "source": [
        "# non Ascii remover\n",
        "remove_features_udf = udf(remove_features, StringType())\n",
        "data = data.withColumn('clean_text',remove_features_udf(data['clean_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dlQfIjGpWK-V"
      },
      "outputs": [],
      "source": [
        "##Fix abbreviations\n",
        "def fix_abbreviation(data_str):\n",
        "    data_str = data_str.lower()\n",
        "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
        "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
        "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
        "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
        "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
        "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
        "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
        "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
        "    data_str = re.sub(r'wtf', 'what the fuck', data_str)\n",
        "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
        "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
        "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
        "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
        "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
        "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
        "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
        "    data_str = re.sub(r'rt\\b', '', data_str)\n",
        "    data_str = data_str.strip()\n",
        "    return data_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2Yg-6zhUWN_T"
      },
      "outputs": [],
      "source": [
        "fix_abbreviation_udf = udf(fix_abbreviation, StringType())\n",
        "data = data.withColumn('clean_text', fix_abbreviation_udf(data['clean_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HUCud6I5dG1O"
      },
      "outputs": [],
      "source": [
        "# Part-of-Speech Tagging\n",
        "def tag_and_remove(data_str):\n",
        "    cleaned_str = ''\n",
        "# noun tags\n",
        "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
        "# adjectives\n",
        "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
        "# verbs\n",
        "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
        "# break string into 'words'\n",
        "    text = data_str.split()\n",
        "# tag the text and keep only those with the right tags\n",
        "    tagged_text = pos_tag(text)\n",
        "    for tagged_word in tagged_text:\n",
        "        if tagged_word[1] in nltk_tags:\n",
        "            cleaned_str = cleaned_str + ' ' + tagged_word[0]\n",
        "    return cleaned_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "64Lwet39dIjp"
      },
      "outputs": [],
      "source": [
        "tag_and_remove_udf = udf(tag_and_remove, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3sPeHTYLdOBc"
      },
      "outputs": [],
      "source": [
        "data = data.withColumn('clean_text', tag_and_remove_udf(data['clean_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9PoXl6pkXMrk"
      },
      "outputs": [],
      "source": [
        "##Lemmatization\n",
        "def lemmatize(data_str):\n",
        "# expects a string\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = data_str.split()\n",
        "    tagged_words = pos_tag(text)\n",
        "    for word in tagged_words:\n",
        "        if 'v' in word[1].lower():\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
        "        else:\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
        "        if list_pos == 0:\n",
        "            cleaned_str = lemma\n",
        "        else:\n",
        "            cleaned_str = cleaned_str + ' ' + lemma\n",
        "        list_pos += 1\n",
        "    return cleaned_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ee1bADhwXdH5"
      },
      "outputs": [],
      "source": [
        "lemmatize_udf = udf(lemmatize, StringType())\n",
        "data = data.withColumn('clean_text', lemmatize_udf(data['clean_text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Pipeline\n",
        "Spark Machine Learning Pipelines API is similar to Scikit-Learn. Our pipeline includes three steps:\n",
        "\n",
        "regexTokenizer: Tokenization (with Regular Expression)\n",
        "\n",
        "stopwordsRemover: Remove Stop Words\n",
        "\n",
        "countVectors: Count vectors (“document-term vectors”)"
      ],
      "metadata": {
        "id": "cYnGvMl2It5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "P5L4wmCd8HDo"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# regular expression tokenizer\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stop_words)\n",
        "\n",
        "# bag of words count\n",
        "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWxNuNBN8HDo",
        "outputId": "fcc5cfa9-6d3f-443d-c47e-5b35919ea271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "|          clean_text|category|               words|            filtered|            features|label|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "|family mormon hav...|       1|[family, mormon, ...|[family, mormon, ...|(9686,[9,11,18,42...|  0.0|\n",
            "|buddhism have muc...|       1|[buddhism, have, ...|[buddhism, much, ...|(9686,[2,5,6,10,1...|  0.0|\n",
            "|don say thing win...|      -1|[don, say, thing,...|[say, thing, win,...|(9686,[0,2,5,10,1...|  2.0|\n",
            "|have learn yours ...|       0|[have, learn, you...|[learn, want, tea...|(9686,[22,148,182...|  1.0|\n",
            "|own benefit want ...|       1|[own, benefit, wa...|[benefit, want, r...|(9686,[6,22,28,49...|  0.0|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
        "\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "dataset.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "\n",
        "# Create p_stemmer of class PorterStemmer\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "texts = [row.filtered for row in dataset.toLocalIterator()]\n",
        "\n",
        "# turn our tokenized documents into a id <-> term dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "    \n",
        "# convert tokenized documents into a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# generate LDA model\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
      ],
      "metadata": {
        "id": "RTpik7z98eHF"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = dataset.toPandas()\n",
        "tweets.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "c0KgGwrHt8Mg",
        "outputId": "6fbcc961-1088-43b6-9e02-3558bba988ca"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          clean_text category  \\\n",
              "0  family mormon have try explain stare puzzle ti...        1   \n",
              "1  buddhism have much lot compatible christianity...        1   \n",
              "2  don say thing win get complex explain normal p...       -1   \n",
              "3  have learn yours yours want teach different fo...        0   \n",
              "4  own benefit want read live live christ thich h...        1   \n",
              "\n",
              "                                               words  \\\n",
              "0  [family, mormon, have, try, explain, stare, pu...   \n",
              "1  [buddhism, have, much, lot, compatible, christ...   \n",
              "2  [don, say, thing, win, get, complex, explain, ...   \n",
              "3  [have, learn, yours, yours, want, teach, diffe...   \n",
              "4  [own, benefit, want, read, live, live, christ,...   \n",
              "\n",
              "                                            filtered  \\\n",
              "0  [family, mormon, try, explain, stare, puzzle, ...   \n",
              "1  [buddhism, much, lot, compatible, christianity...   \n",
              "2  [say, thing, win, get, complex, explain, norma...   \n",
              "3  [learn, want, teach, different, focus, goal, w...   \n",
              "4  [benefit, want, read, live, live, christ, thic...   \n",
              "\n",
              "                                            features  label  \n",
              "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0  \n",
              "1  (0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, ...    0.0  \n",
              "2  (1.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, ...    2.0  \n",
              "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0  \n",
              "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...    0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63df7327-022c-4104-9440-51c7699bcb7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>category</th>\n",
              "      <th>words</th>\n",
              "      <th>filtered</th>\n",
              "      <th>features</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>family mormon have try explain stare puzzle ti...</td>\n",
              "      <td>1</td>\n",
              "      <td>[family, mormon, have, try, explain, stare, pu...</td>\n",
              "      <td>[family, mormon, try, explain, stare, puzzle, ...</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>buddhism have much lot compatible christianity...</td>\n",
              "      <td>1</td>\n",
              "      <td>[buddhism, have, much, lot, compatible, christ...</td>\n",
              "      <td>[buddhism, much, lot, compatible, christianity...</td>\n",
              "      <td>(0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>don say thing win get complex explain normal p...</td>\n",
              "      <td>-1</td>\n",
              "      <td>[don, say, thing, win, get, complex, explain, ...</td>\n",
              "      <td>[say, thing, win, get, complex, explain, norma...</td>\n",
              "      <td>(1.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>have learn yours yours want teach different fo...</td>\n",
              "      <td>0</td>\n",
              "      <td>[have, learn, yours, yours, want, teach, diffe...</td>\n",
              "      <td>[learn, want, teach, different, focus, goal, w...</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>own benefit want read live live christ thich h...</td>\n",
              "      <td>1</td>\n",
              "      <td>[own, benefit, want, read, live, live, christ,...</td>\n",
              "      <td>[benefit, want, read, live, live, christ, thic...</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63df7327-022c-4104-9440-51c7699bcb7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-63df7327-022c-4104-9440-51c7699bcb7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-63df7327-022c-4104-9440-51c7699bcb7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "topic_distributions = []\n",
        "\n",
        "for index, row in tweets.iterrows():\n",
        "    tokens = word_tokenize(row['clean_text'].lower())\n",
        "    bow = dictionary.doc2bow(tokens)\n",
        "    topic_distribution = ldamodel.get_document_topics(bow)\n",
        "    topic_distributions.append(topic_distribution)\n",
        "    \n",
        "topic_probs = []\n",
        "\n",
        "for distribution in topic_distributions:\n",
        "    topic_probs.append([(topic, prob) for topic, prob in distribution])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTCkj3Rs0dQ",
        "outputId": "59d100f6-027d-401f-972c-f167f5ac4a39"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_topics = {}\n",
        "for i in range(len(topic_probs)):\n",
        "  for (topic, prob) in topic_probs[i]:\n",
        "    distinct_topics[topic] = 1\n",
        "print(len(distinct_topics), ldamodel.num_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rpA2xyKtAV4",
        "outputId": "09457328-1796-49af-9f28-cc67fa8bf148"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7U4iuA8HDo"
      },
      "source": [
        "## Partition Training & Test sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, lit\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "import pandas as pd\n",
        "\n",
        "# Set threshold for probability\n",
        "threshold = 0.05\n",
        "\n",
        "# Create dictionary to store trained models for each subset and topic\n",
        "model_dict = {}\n",
        "\n",
        "# Iterate through each topic\n",
        "for topic_id in range(ldamodel.num_topics):\n",
        "\n",
        "    # Select tweets with high probability for current topic\n",
        "    tweets_subset = pd.DataFrame(columns = ['clean_text', 'category', 'words', 'filtered', 'features', 'label'])\n",
        "    for index, row in tweets.iterrows():\n",
        "        tweet_bow = dictionary.doc2bow(row['clean_text'].lower().split())\n",
        "        topic_prob = ldamodel.get_document_topics(tweet_bow, minimum_probability=0.0)\n",
        "        topic_prob = dict(topic_prob)\n",
        "        if topic_id in topic_prob and topic_prob[topic_id] > threshold:\n",
        "          tweets_subset = tweets_subset.append(row, ignore_index=True)\n",
        "    \n",
        "    # Create training subset\n",
        "    if tweets_subset.shape[0] > 0:\n",
        "        pyspark_tweets_subset = sqlContext.createDataFrame(tweets_subset)\n",
        "        \n",
        "        ml_model = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "        trained_model = ml_model.fit(pyspark_tweets_subset)\n",
        "\n",
        "        # Store trained model in dictionary\n",
        "        model_dict[topic_id] = trained_model"
      ],
      "metadata": {
        "id": "jfOncOQyAJfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f0b588-708f-4c76-f1fa-734e3b220f52"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-239-eb11db525e75>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  tweets_subset = tweets_subset.append(row, ignore_index=True)\n",
            "<ipython-input-239-eb11db525e75>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  tweets_subset = tweets_subset.append(row, ignore_index=True)\n",
            "<ipython-input-239-eb11db525e75>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  tweets_subset = tweets_subset.append(row, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(topic_probs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4slwP7Me5O8",
        "outputId": "ce7b0316-1cc7-4e78-cdf8-4bb5e2cd6008"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.060553323), (1, 0.9120973), (2, 0.027349388)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_labels = {0: \"Positive\", 1: \"Neutral\", 2: \"Negative\"}"
      ],
      "metadata": {
        "id": "TwrrU05nLUVr"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define sentiment estimation function\n",
        "def estimate_sentiment(tweet, threshold=0.05):\n",
        "  \n",
        "  input_data = pd.DataFrame({\"clean_text\": [tweet]})\n",
        "  tweet_df = sqlContext.createDataFrame(input_data)\n",
        "\n",
        "  # Extract features using HashingTF and IDF\n",
        "  regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "  tokenized_data = regexTokenizer.transform(tweet_df)\n",
        "  hashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures')\n",
        "  featurized_data = hashing_tf.transform(tokenized_data)\n",
        "  idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
        "  idf_model = idf.fit(featurized_data)\n",
        "  rescaled_data = idf_model.transform(featurized_data)\n",
        "  # initialize empty array for sentiment probabilities\n",
        "  sentiment_probs = [0] * ldamodel.num_topics\n",
        "  \n",
        "  sentiments = []\n",
        "  for topic in range(ldamodel.num_topics):\n",
        "    sentiments.append(model_dict[topic].transform(rescaled_data))\n",
        "\n",
        "  #Get topics related to the tweet\n",
        "  tweet_bow = dictionary.doc2bow(tweet.lower().split())\n",
        "  topic_prob = ldamodel.get_document_topics(tweet_bow, minimum_probability=0.0)\n",
        "  topic_prob = dict(topic_prob)\n",
        "\n",
        "  for topic, prob in topic_prob.items():\n",
        "    if prob > threshold:\n",
        "      sentiment_probs[topic] = sentiments[topic].select(\"prediction\").collect()[0][0]\n",
        "\n",
        "  # set sentiment estimation to the one with highest probability\n",
        "  cnt = {}\n",
        "  for num in sentiment_probs:\n",
        "    if num not in cnt:\n",
        "      cnt[num] = 1\n",
        "    else:\n",
        "      cnt[num] += 1\n",
        "  \n",
        "  mode = -1\n",
        "  maxValue = -1\n",
        "  for key, value in cnt.items():\n",
        "    if(value > maxValue):\n",
        "      maxValue = value\n",
        "      mode = key\n",
        "  \n",
        "  sentiment_label = sentiment_labels.get(int(mode), \"Unknown\")\n",
        "  return sentiment_label"
      ],
      "metadata": {
        "id": "rf-htP4BNDac"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = estimate_sentiment(\"I am awesome!\", 0.05)\n",
        "print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG5L8MH9P2X0",
        "outputId": "1c059b35-a4fe-404b-eb28-2f04020bbd1b"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOm9t38a8HDo",
        "outputId": "dc4d0fbd-ac6b-4c4a-9a7c-e48d16c80010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 25892\n",
            "Test Dataset Count: 10996\n"
          ]
        }
      ],
      "source": [
        "# set seed for reproducibility\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06oK8ipY8HDo"
      },
      "source": [
        "## Model Training and Evaluation\n",
        "Logistic Regression using Count Vector Features \n",
        "\n",
        "Our model will make predictions and score on the test set; we then look at the top 10 predictions from the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GSBM7jB8HDo",
        "outputId": "37f169bc-a5eb-4f04-965f-88c721bb0ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|iit iim student be much dif...|       1|[0.9999999999999931,2.60521...|  0.0|       0.0|\n",
            "|ist aht teh chikcf rom retr...|       1|[0.9999998357271883,5.40996...|  0.0|       0.0|\n",
            "|chennai super king ipl fini...|       1|[0.9999997571296896,1.19526...|  0.0|       0.0|\n",
            "|svt murder case except culp...|       1|[0.9999992589312988,1.03962...|  0.0|       0.0|\n",
            "|draft few point frustration...|       1|[0.9999989701361153,2.91350...|  0.0|       0.0|\n",
            "|lol acting be good good goo...|       1|[0.9999980311801355,5.06028...|  0.0|       0.0|\n",
            "|chennai super king season c...|       1|[0.9999979769953309,1.21011...|  0.0|       0.0|\n",
            "|couple reason see though ha...|       1|[0.9999950052385942,8.21087...|  0.0|       0.0|\n",
            "|author conclusion buddhism ...|       1|[0.999994987589762,5.486006...|  0.0|       0.0|\n",
            "|article adblocker follow te...|       1|[0.9999946757139386,3.67913...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\")\\\n",
        ".orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2t87Hsl8HDo",
        "outputId": "7ca24a77-9ce1-4ee6-e950-c0b5862184e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7117525129668887"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subset partitioning training result for N = 1."
      ],
      "metadata": {
        "id": "-i8EYP30YWvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model_dict[0].transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\")\\\n",
        ".orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiojwAimMJ-M",
        "outputId": "99a230ad-1ac2-4e26-ccb1-a79f23c0ace0"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|amazonize state bid india i...|       1|[0.9999999927096903,8.04325...|  0.0|       0.0|\n",
            "|go churlish puncture baloon...|       1|[0.9999999848105883,4.39544...|  0.0|       0.0|\n",
            "|site attack put modi strong...|       1|[0.9999999788968086,4.57450...|  0.0|       0.0|\n",
            "|people have right know sour...|       1|[0.9999999514236283,3.95174...|  0.0|       0.0|\n",
            "|holy shit gand muslim gangs...|      -1|[0.9999999188591101,2.47698...|  2.0|       0.0|\n",
            "|many point such welfare edu...|       1|[0.9999999179972027,2.63643...|  0.0|       0.0|\n",
            "|first time be great gesture...|       1|[0.999999885852927,1.060683...|  0.0|       0.0|\n",
            "|author conclusion buddhism ...|       1|[0.9999998071385555,8.71114...|  0.0|       0.0|\n",
            "|svt murder case except culp...|       1|[0.9999996887338601,5.89805...|  0.0|       0.0|\n",
            "|dhruv frustration misinform...|       1|[0.9999996643673738,3.62970...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7549026625358487"
            ]
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subset partitioning training result for N = 3."
      ],
      "metadata": {
        "id": "EdJ8HF13YoN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for i in range(3):\n",
        "  predictions.append(model_dict[i].transform(testData))"
      ],
      "metadata": {
        "id": "1CCG9heLYp3Y"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mode\n",
        "\n",
        "for i in range(3):\n",
        "  predictions[i].select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\")\\\n",
        ".orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ego3XF4vZKU-",
        "outputId": "308722ba-6964-43c7-d425-b678d14049d4"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|amazonize state bid india i...|       1|[0.9999999927096903,8.04325...|  0.0|       0.0|\n",
            "|go churlish puncture baloon...|       1|[0.9999999848105883,4.39544...|  0.0|       0.0|\n",
            "|site attack put modi strong...|       1|[0.9999999788968086,4.57450...|  0.0|       0.0|\n",
            "|people have right know sour...|       1|[0.9999999514236283,3.95174...|  0.0|       0.0|\n",
            "|holy shit gand muslim gangs...|      -1|[0.9999999188591101,2.47698...|  2.0|       0.0|\n",
            "|many point such welfare edu...|       1|[0.9999999179972027,2.63643...|  0.0|       0.0|\n",
            "|first time be great gesture...|       1|[0.999999885852927,1.060683...|  0.0|       0.0|\n",
            "|author conclusion buddhism ...|       1|[0.9999998071385555,8.71114...|  0.0|       0.0|\n",
            "|svt murder case except culp...|       1|[0.9999996887338601,5.89805...|  0.0|       0.0|\n",
            "|dhruv frustration misinform...|       1|[0.9999996643673738,3.62970...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|amazonize state bid india i...|       1|[0.9999991982980573,3.11632...|  0.0|       0.0|\n",
            "|chennai super king season c...|       1|[0.9999988943178202,3.03748...|  0.0|       0.0|\n",
            "|lol acting be good good goo...|       1|[0.9999984791887913,3.76259...|  0.0|       0.0|\n",
            "|lola whore best overall pos...|       1|[0.9999970070603352,4.77372...|  0.0|       0.0|\n",
            "|real dick best overall subm...|       1|[0.9999969227873968,1.68981...|  0.0|       0.0|\n",
            "|illiterate best overall sub...|       1|[0.9999956751822088,2.50465...|  0.0|       0.0|\n",
            "|india best overall submitte...|       1|[0.9999956450401046,2.78851...|  0.0|       0.0|\n",
            "|lot best overall submitter ...|       1|[0.999995365219225,3.037470...|  0.0|       0.0|\n",
            "|be lot best overall submitt...|       1|[0.999995203796816,3.150885...|  0.0|       0.0|\n",
            "|bhagina vagina best overall...|       1|[0.9999933591910698,3.89656...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|india best overall submitte...|       1|[0.9999999999999998,4.61727...|  0.0|       0.0|\n",
            "|lot best overall submitter ...|       1|[0.9999999999999998,4.58613...|  0.0|       0.0|\n",
            "|illiterate best overall sub...|       1|[0.9999999999999998,4.45719...|  0.0|       0.0|\n",
            "|bhagina vagina best overall...|       1|[0.9999999999999998,3.71393...|  0.0|       0.0|\n",
            "|real dick best overall subm...|       1|[0.9999999999999998,2.73952...|  0.0|       0.0|\n",
            "|be lot best overall submitt...|       1|[0.9999999999999996,1.59797...|  0.0|       0.0|\n",
            "|steel live band family memb...|       1|[0.9999999999999956,2.55847...|  0.0|       0.0|\n",
            "|go churlish puncture baloon...|       1|[0.9999999992316477,2.18943...|  0.0|       0.0|\n",
            "|get pattern ego ego ego ego...|       0|[0.9999999971586024,2.84139...|  1.0|       0.0|\n",
            "|lola whore best overall pos...|       1|[0.9999999609018959,1.13481...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_predictions = predictions[0].union(predictions[1]).union(predictions[2])"
      ],
      "metadata": {
        "id": "z630HFCjgYKg"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import first\n",
        "result_predictions = combined_predictions.groupBy('clean_text') \\\n",
        "                      .agg(mode(col('prediction')).alias('mode_prediction'),\n",
        "                           first(col('category')).alias('category'),\n",
        "                           first(col('label')).alias('label'))\n",
        "result_predictions.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHmxb7YZgiHu",
        "outputId": "37052d84-eef5-46c3-fd7e-09c00357bc97"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+--------+-----+\n",
            "|          clean_text|mode_prediction|category|label|\n",
            "+--------------------+---------------+--------+-----+\n",
            "|   aaaand streak end|            1.0|       0|  1.0|\n",
            "|aadhaar privacy i...|            0.0|       1|  0.0|\n",
            "|              aadhar|            1.0|       0|  1.0|\n",
            "|aadhar exasperate...|            1.0|       0|  1.0|\n",
            "|aadhar secure sec...|            0.0|       1|  0.0|\n",
            "+--------------------+---------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol='mode_prediction')\n",
        "evaluator.evaluate(result_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yjtFR7BhOZ4",
        "outputId": "07e50dd0-8c2c-4527-e873-400fcbf7b179"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7893339287131915"
            ]
          },
          "metadata": {},
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PaB1QR88HDp"
      },
      "source": [
        "## Logistic Regression using TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaJ9o8Yd8HDp",
        "outputId": "aa0804f2-7c39-4db8-d0f3-86d7f0cf6225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|iit iim student be much dif...|       1|[0.9999999999950364,1.92636...|  0.0|       0.0|\n",
            "|chennai super king ipl fini...|       1|[0.9999999997584104,3.77359...|  0.0|       0.0|\n",
            "|ist aht teh chikcf rom retr...|       1|[0.9999998028760817,5.94457...|  0.0|       0.0|\n",
            "|lol acting be good good goo...|       1|[0.9999981055490093,4.86939...|  0.0|       0.0|\n",
            "|svt murder case except culp...|       1|[0.9999962708666431,1.02497...|  0.0|       0.0|\n",
            "|author conclusion buddhism ...|       1|[0.9999952314738392,1.12138...|  0.0|       0.0|\n",
            "|be good good good good good...|       1|[0.9999934857018989,1.68422...|  0.0|       0.0|\n",
            "|actor be good good good goo...|       1|[0.9999921351462316,2.06716...|  0.0|       0.0|\n",
            "|think same result be good g...|       1|[0.9999908593788321,2.36246...|  0.0|       0.0|\n",
            "|article adblocker follow te...|       1|[0.9999893849987291,7.98956...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=30000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d0FInj18HDp",
        "outputId": "c779bd32-04e0-46ca-e192-cb7c832efe3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6937969783479848"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGM3Ytrl8HDp"
      },
      "source": [
        "## Cross-Validation\n",
        "Let’s now try cross-validation to tune our hyper parameters, and we will only tune the count vectors Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njSz2rZr8HDp"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC_kzktE8HDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56dce04-0083-4b2b-d5af-b40dafcad6b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.752211153632236"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Create ParamGrid for Cross Validation\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
        "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
        "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
        "             .build())\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "cv = CrossValidator(estimator=lr, \\\n",
        "                    estimatorParamMaps=paramGrid, \\\n",
        "                    evaluator=evaluator, \\\n",
        "                    numFolds=5)\n",
        "\n",
        "cvModel = cv.fit(trainingData)\n",
        "\n",
        "predictions = cvModel.transform(testData)\n",
        "# Evaluate best model\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)\n",
        "#print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9X9O_Tb8HDp"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x7gjw_X8HDp",
        "outputId": "09e6bff3-cc05-46f1-ca21-e896213a2af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|respect sir sar madam sar g...|       1|[1.0,9.182746314341452E-17,...|  0.0|       0.0|\n",
            "|respect sir sar madam good ...|       1|[1.0,1.7338009126881107E-17...|  0.0|       0.0|\n",
            "|respect sar ayam subash sar...|       1|[1.0,1.6868856139650975E-17...|  0.0|       0.0|\n",
            "|respect sir sar madam ayam ...|       1|[1.0,5.650218080012626E-18,...|  0.0|       0.0|\n",
            "|team be gun srh kane willia...|       1|[1.0,1.0491190511005182E-18...|  0.0|       0.0|\n",
            "|csk dhoni raina be support ...|       1|[1.0,3.7655437920326383E-19...|  0.0|       0.0|\n",
            "|team be gun srh start back ...|       1|[1.0,1.219565138353032E-19,...|  0.0|       0.0|\n",
            "|team be gun toss csk srh ta...|       1|[1.0,1.2271904441956659E-20...|  0.0|       0.0|\n",
            "|delhi daredevil major reaso...|       1|[1.0,6.926153491620596E-21,...|  0.0|       0.0|\n",
            "|brown skin beauty hot curvy...|       1|[1.0,9.522767874399856E-25,...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb = NaiveBayes(smoothing=1)\n",
        "model = nb.fit(trainingData)\n",
        "predictions = model.transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlL-MGgI8HDp",
        "outputId": "16cf6b7a-b198-4ca1-c985-f20484e14f5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7002298764346058"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLxZjOTg8HDp",
        "outputId": "a7e38f44-bc6b-4109-8a4e-79cf9663025a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|agree everyone get somethin...|      -1|[0.9114864864864864,0.00270...|  2.0|       0.0|\n",
            "|accord be good reliable sou...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "|bhechodh mirgi diya aur bol...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "|agree say fail state countr...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "|agree people give vote basi...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "|accord role lose arm tyr fe...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "|damn rahul good lookin dude...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "|accurate learn high school ...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "|arnabs last election interv...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "|actor be good good good goo...|       1|[0.9114864864864864,0.00270...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
        "dtModel = dt.fit(trainingData)\n",
        "predictions = dtModel.transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4TjiBMc8HDp",
        "outputId": "b121a6b1-41c0-428f-ff06-b589f23ed984"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.27266082368144756"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSVO12FC8HDq"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kcicq9CE8HDq",
        "outputId": "09ce9c78-d876-43e2-ae28-cdc2b0ac1a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|iit iim student be much dif...|       1|[0.6153750155588668,0.13838...|  0.0|       0.0|\n",
            "|brilliant point point other...|       1|[0.596061641963918,0.162264...|  0.0|       0.0|\n",
            "|draft few point frustration...|       1|[0.5884963895732044,0.19201...|  0.0|       0.0|\n",
            "|be simpleton have be victim...|       1|[0.5743760151164825,0.15607...|  0.0|       0.0|\n",
            "|dhruv frustration misinform...|       1|[0.5570935262753622,0.23898...|  0.0|       0.0|\n",
            "|spent sta afghanistan first...|       1|[0.5564944588299858,0.18814...|  0.0|       0.0|\n",
            "|bjp different other politic...|       1|[0.5558701803561679,0.19732...|  0.0|       0.0|\n",
            "|chennai super king season c...|       1|[0.5552815543887953,0.22082...|  0.0|       0.0|\n",
            "|see pitch raise argument as...|      -1|[0.5530906094382828,0.22041...|  2.0|       0.0|\n",
            "|think lot truth say want ad...|       1|[0.5514194435784436,0.22328...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
        "                            featuresCol=\"features\", \\\n",
        "                            numTrees = 100, \\\n",
        "                            maxDepth = 4, \\\n",
        "                            maxBins = 32)\n",
        "# Train model with Training Data\n",
        "rfModel = rf.fit(trainingData)\n",
        "predictions = rfModel.transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enr4SECO8HDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fe878a-5e4e-4acc-cb07-8b54c2d8defa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27207601721338165"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
        "csv_stream = spark.readStream.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"maxFilesPerTrigger\", 1) \\\n",
        "    .schema(customSchema) \\\n",
        "    .load(filename1)"
      ],
      "metadata": {
        "id": "uPaxB-NFDcbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}